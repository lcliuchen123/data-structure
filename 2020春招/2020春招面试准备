一、自我介绍
    你好，我叫刘晨，是中南财经政法大学数理统计学的硕士研究生。

    在校期间主要学习了统计学的相关课程，因为对数据建模比较感兴趣，主动报名参加了东证期货杯和华为杯建模比赛。
    东证期货杯主要是利用贷款者的基本信息，信用记录和收入水平进行建模，预测是否逾期。
    华为杯主要是利用死伤人数，财产损失，武器类型等指标对恐怖事件的危害程度进行定级。
    另外，主动学习了sql，R语言和python等专业技能

    从去年11月份开始，先后在字节跳动，平安科技和小浆科技实习，实习内容基本都是与数据相关的，
    与我们公司岗位需求比较符合。

    平时有空的时候比较喜欢打篮球和游泳。

    以上是我的自我介绍，总体而言，我觉得自己比较适合这份工作。请问还有什么要问的？

二、项目经历
    智评系统研发
    主要内容：主要分为成绩展示、学生测评、协同分组和智能推荐四个模块，主要服务对象是家长和老师。
    负责内容：
        1. 学生测评：利用项目反应理论挖掘学生对每个知识点的掌握程度，简单说就是利用学生做题的得分矩阵衡量。
        2. 试题-知识点匹配：关键词匹配--利用word2vec生成词向量，logistic和svm进行分类

三、实习经历
 （1）今日头条
    实习地点：北京牡丹园
    实习公司：字节跳动旗下的今日头条
    实习时间：2018年11月-2019年1月
    实习内容：
        1. 退款分析维度：各个城市的退款金额，已消耗金额，退款率，退款周期，退款原因。
        2. 参与设计问卷，进行全量回访，了解客户的真实退款原因。
        3. 首先选取的聚类指标：用户所在地区，行业类别，充值时间，已消耗金额，充值金额，充值时长，上线时间。
           利用K均值聚类将退款用户分为5类，然后选取退款金额和上线时间进行加权求和，对每类退款客户进行定级。
           计算未退款用户跟每个用户之间的距离，选择距离近的交给退款挽回系统。

 （2）平安科技
    命名实体识别项目
    实习地点：上海崂山路
    实习公司：平安科技（深圳）有限公司上海分部
    实习时间：2019年2月-2019年6月
    实习内容：
        1. 清洗数据。
        2. 命名实体识别项目，BERT+正则表达式。利用正则表达式匹配邮箱学校名称这些。
        输入是一个txt,把PDF文件转化为txt，pdfminer，OCR（crnn）
        3. 金融文本识别，BERT+BILSTM+CRF。
        问题：
            1.为什么要加BiLSTM?
            答：补充顺序信息
            2.crf不是特别了解。 过程，求解
            crf是一种图模型，添加一些固定的约束
    百度的ERNIE（知识增强的语义表示模型）

 （3）小浆科技
    推荐数据分析
    实习地点：北京望京
    实习公司：小浆科技
    实习时间：2019-07-16-2019-08-26
    实习内容：
        1.在superset上配置报表（sql),留存，新增用户数，人均观看时长，人均观看次数，语音数，点赞数，分享数等核心指标。
        2.编写notebook脚本日报，利用python连接mysql，hive，presto。
        3.监测拉新、留存等核心指标。分析AB实验的效果，验证推送策略是否可行。
        埋点事件：app_start,ad_show,client_show,video_play,video_effective_play,video_over,share,audio,like

四、机器学习
    （1）logitis与SVM的联系和区别：
         1. 联系：线性模型，有监督学习算法，分类模型，判别模型
         2. 区别：
                2.1 损失函数不同：对数似然损失和基于几何间隔最大化的损失函数
                2.2 svm只考虑支持向量，logistic会考虑所有的点
                2.3 解决非线性问题时，svm会利用核函数
                2.4 svm基于距离，LR基于概率，所以svm受维度的影响
                2.5 SVM自带结构风险最小化，LR则是经验风险最小化
         3. 在实际应用中，小数据集更适合svm，大数据集更适用于logistic,因为logistic的结果解释性比较强，表示属于某个类别的概率
    （2）随机森林
         1.随机选择样本，随机选择特征
         2.如何衡量特征重要性？
           2.1 引入袋外误差，判断引入噪声后该特征对模型误差的影响
           2.2 基尼指数：对分裂前后的基尼指数变化量进行排序，分裂前的基尼指数- （分裂后的左子树的基尼指数+分裂后的基尼指数）
    （3）GBDT与xgboost的区别
         1. GBDT只支持CART回归树，xgboost支持其它分类器
         2. GBDT只用了一阶导数，xgboost用了二阶展开
         3. 引入正则项，避免过拟合
         4. 引入列采样， 降低过拟合，减少计算量
         5. xgboost支持并行处理，在特征上并行，对特征排序后以block形式存储，方便在后面调用。
    （4）GBDT与xgboost的区别：
         1. RF可以是分类树也可以是回归树，GBDT只能是cart回归树
         2. RF对异常值不敏感，因为多棵树表决， 而GBDT比较敏感，当前的错误会传给下一棵树
         3. RF减少方差，GBDT减少偏差
         4. 随机森林不需要归一化，GBDT需要。
    （5）生成模型和判别模型
        1.生成模型：学习得到联合概率分布P(x,y)，即特征x和标记y共同出现的概率，然后求条件概率分布。能够学习到数据生成的机制。
          常见的有朴素贝叶斯，隐马尔可夫链，混合高斯模型。
        2.判别模型：学习得到条件概率分布P(y|x)，即在特征x出现的情况下标记y出现的概率。
          常见的有logistic回归、svm和条件随机场等模型
    （6）knn：手写代码
    （7) k均值聚类：手写代码
    （8）HMM
     (9) CRF

五、深度学习
    (1)cnn
        最大池化适用于比较浅的网络，平均池化适用于比较深的网络
        卷积的物理意义是什么？---一个函数在另一个函数上的加权叠加
    (2)rnn
    (3)lstm
    (4)gru
    (5)word2vec
    (6)Transformer
    (7)BERT
   （8）梯度消失和梯度爆炸的解决方法
      更换激活函数
      梯度剪切
      batch norm
      残差连接
      添加正则项


六、数据库
    1.常用的sql语句
      1.1 sql中的主键
          数据库中的主键是一列或多列的组合。
          一个表只能有一个主键，一个主键可以由多列组成。
          主键是用来唯一标识每一行的，不能重复，也不能有空值。
      1.2 创建表
        create table products
        (
            pro_id int primiary key AUTO_INCREMENT comment 'id',
            vend_id int not null comment '',
            produc_name VARCHAR(128) COMMENT '',
            price decimal（8,2） comment '价格'，#8表示整数部分加小数部分长度不能超过8位，小数部分2位。
            create_time datetime default current_timestamp comment '创建时间',
            update_time datetime comment '更新时间'
        );
        删除列：alter table table_name drop column column_name
        添加列：alter table table_name add column column_name
        重命名列名：alter table name change column name varchar(100)
        重命名表名：rename table_name people to new_name
    2.sql语句的执行顺序
      开始--from--on--join--where--group by--avg,sum,max,min--having--select--distinct--order by--结束
    3.sql中的注意点，参考网址https://zhuanlan.zhihu.com/p/101258002?utm_source=wechat_session：
      3.1 where group by having：having只能用在group by后面，where子句不能与聚合函数连用，但是having可以
      3.2 count distinct: 去重
      3.3 inner join: 求交集
      3.4 outer join: 求左边表和满足条件的部分右表的数据
      3.5 case when 条件1 then 结果1 else 结果2
      3.6 rank排序函数：RANK ( ) OVER ( [ PARTITION BY <partition_column> ] ORDER BY <order_by_column> )
      3.7 union与union all 用来合并两张临时表，union会自动去除重复值，两张表要有相同的列。
      3.8 between and是查询范围，闭区间，经常用来限定日期范围
      3.9 like 通配符：模糊查询
    4.优化sql语句的技巧？
      4.1 避免使用select *
      4.2 利用explain进行解析
      4.3 避免使用not in, <>操作符
      4.4 尽量利用union代替union all
      4.5 like模糊匹配时不要在开始加通配符，这样会全盘扫描，为什么？--因为索引是利用B+树建立的
      4.6 利用union代替or
      4.7 使用left join代替子查询
      4.8 不要在where子句中的“=”左边进行函数、算数运算或其他表达式运算，否则系统将可能无法正确使用索引。
    5.对sql索引的理解，索引是不是越多越好？
      5.1 当然不是，索引也会占用一定的空间
          插入和更新也会消耗的一定资源
      5.2 联合索引按照最左匹配原则排序，即从左到右排序
      5.3 无法使用索引的几种场景
          对索引列进行函数计算
          not in <>操作符
          like 通配符前面加%
          select *
          隐式转换
    6.hive
        6.1 什么是hive？
           hive是一款开源的基于Hadoop的用于统计海量结构化数据的一个数据仓库，定义了类sql查询语言，HQL。
           本质上讲hive是将HQL语句转换成MapReduce的一个工具。
        6.2 什么是数据仓库？
           数据库是按照数据结构来组织、存储
           数据仓库是一个面向主题的，集成的，相对稳定的，反映历史变化的数据集合。用于支持管理决策。
        6.3 简单说下MapReduce？
            一个计算框架，能够容易的编写应用程序，这些程序能够运行在大规模集群上，并以一种可靠地，具有容错能力的
            的方式并行处理TB以上的海量数据集。
            主要思想就是分而治之，mapper负责分，把复杂任务分解成若干个简单的简单的任务；
            reducer人负责对map阶段的结果进行汇总。
        6.4 hive的作用
            1.可以将结构化的数据文件映射成一张表，并提供类sql查询功能，方便非java开发人员进行操作。
            2.可以对数据提取转化加载（ETL)。
            3.构建数据仓库。
        6.5 hive的使用场景
            1.即席查询：利用CLI和Hue之类的工具，可以对hive的数据做即席查询。
            2.离线数据分析：通过脚本执行HQL语句。
            3.构建数据仓库时用于组织管理数据库和表。
        6.6 hive的构成部分
            1.用户接口层：CLI（命令行模式）,JDBC/ODBC和WUI。
              CLI最常用，启动的时候会启动一个hive副本。
              JDBC/ODBC是hive的客户端，用户通过客户端连接到hive server。
              客户端模式需要指出hive server所在节点，在该节点启动hive server。
              WUI是通过浏览器访问hive。
            2.元数据存储
              元数据包括Database，表名，存储空间，分区、表数据所在目录等。
              存储位置：RDSMS.（mysql、derby)
              三种连接模式：内嵌式元存储服务，本地元存储服务器和远程元存储服务器。
            3.Driver（Compiler/Optimizer/Executor)
              Driver完成HQL查询语句的词法分析，语法分析，编译，优化以及查询计划的生成。
              生成的查询计划存储在HDFS上，并由MapReduce调用执行。
    7. 数据倾斜
        1.什么是数据倾斜？
            数据分布不均匀，即大量相同的key被分到相同的分区中。造成了部分节点堵塞，忙得忙死，闲的闲死。

        2.为什么会产生数据倾斜？
            数据分布不均匀，大部分倾斜的原因都在shuffle（数据从map端传到reduce端：分区，排序，合并）这个过程。

            1.读入数据的时候就是倾斜的，如读取id分布跨度较大的mysql数据、partition分配不均的kafka数据或不可分割的压缩文件。
            2.shuffle产生倾斜，如sql语句：group by,join,count(distinct)这些都会引发shuffle。
            3.过滤导致倾斜，数据本身平衡，经过一系列操作时倾斜。

        3.解决数据倾斜的方法？
            1.尽量保证数据源均匀。
            2.对大数据集过滤后，进行repartition.
            3.优化sql代码
            先group by再count替代count（distinct)
            对于分布不均匀的数据单独计算，比如推广活动造成部分城市订单量猛增，单独计算这些城市的数据。

            网易：
            1.skewTune： 监测每个map的执行时间，将耗时较长的分配到其他节点。
            2.skewReduce :用户可以自定义分区函数，优化数据分区。
            3.LIBRA:随机部分map任务，估计数据分布，采用不同的分区。map阶段
            4.LEEN: 按照频率和分布的均匀程度，重新设计key. shuffle阶段

        4.实际中如何检测是否数据倾斜并采取合理的解决方法？
            1.某个reduce task，卡在99.9%半天不动。
            2.任务超时被杀掉。
            3.如果某个reduce的时间比其他reduce时间长的多。

    8.MapReduce的执行过程
            1.输入HDFS中的数据块（BLOCK)，利用job描述计算maptask的数量。
            2.数据切片（只是对单一数据进行切片，对每个单一的文件进行切片），输入键值对，利用map()方法，进行逻辑运算。
            3.进入shuffle阶段，利用key进行分区排序，传递给reducer.
            4.归并排序，相同的k一组，调用reduce()方法进行逻辑运算
            额外信息：
            排序是Hadoop的灵魂。
            1.Hadoop的四大组件：HDFS,MapReduce,YARN,common(底层支撑组件)
            2.map，reduce，Driver负责提交job对象。shuffle是map的输出到reduce的输入过程中进行的分区，排序。
            3.map端输入键值对，输出键值对，不断地进行分区，排序，最终合并成一个大文件，传递给Reduce端。

    9. 为什么key-value的查询时间是O（1）？
       字典的底层结构是散列表，查询次数的期望值是1/（1-alpha），只要存储效率（alpha）不超过50%，查询次数就不会大于2.

七、在线编码
