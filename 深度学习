
1.激活函数的选择
    理论上只要是连续可导的函数都可以作为激活函数。
    常用的激活函数：sigmoid函数和relu函数。
    sigmoid：不是关于原点中心对称，梯度下降法求最优解时，呈现之字形变化，收敛速度较慢。
             需要进行幂运算，消耗时间。
             容易达到饱和。(-10,10)，容易造成梯度消失。
    relu: y = max(0,x)
    可以解决梯度消失的问题，收敛速度较快。

2.梯度下降法的对比（adm,mini-batch) adm不理解
    以平方损失为例，梯度下降是对所有的样本求平均进行进行梯度更新。
    随机梯度下降：每次用一个样本近似所有的样本，进行梯度更新。
        优点：1.加入了噪声，提高了泛化误差。
              2.计算速度快。
        缺点：1.不收敛，在最小值附近波动。
              2.容易陷入局部最小值点。
    mini-batch梯度下降：随机选择部分样本进行更新，要对学习率衰减，刚开始希望步长较大，后面希望变小。

3.batchsize的选择，是不是越大越好？
    不是，较大的batchsize计算速度加快，容易使模型收敛在局部极小值点；
    较小的batch相当于认为加入噪声，计算速度比较慢。
    一般来说batchsize越大，其梯度下降的方向越准确，训练振荡越小。但是收敛效果好，不代表泛化性能一定好。
    batchsize越大，处理数据所需时间越短，但是达到相同精度需要的epoch更多。

    问题：减少batchsize能够降低过拟合吗？
    可以降低过拟合，减少batchsize，梯度下降的方向不是特别准确，收敛效果可能不会达到最优，但是会降低过拟合。

4.梯度爆炸和梯度消失
    原因：1.权重初始值设置值过大。
          2.网络太复杂，反向传播用到链式法则。
          3.激活函数的选择不合适。
    解决方法：
          1.权重正则化，添加正则项。
          2.梯度剪切，设置梯度阈值。
          3.更换激活函数。
          4.利用LSTM的门结构。
          5.预训练加微调（少用）

5.神经网络过拟合的原因及解决的方法
    原因：
        1. 参数过多并且过训练。
        2. 数据中存在噪声，与真实数据不符。
        3. 数据量太小，无法揭示真实的数据分布。
        4. 迭代次数过多

    解决方法：
        1. early stooping
        2. dropout
        3. 增大样本量
        4. 交叉验证
        5. 正则化，如L2正则（权重衰减）

6. 为什么RNN会梯度消失或者发散？
    BPTT反向传播的链式法则
    初始值一般都比较小，a1+a1*a2+a1*a2*a3+......,
    随着层数增加，最终会趋向0。 如果初始值较大，最终会趋向无穷。
    但是初始时刻的梯度并不会消失，RNN的梯度消失是指无法学习到长距离的依赖关系，并不是无法学习短距离依赖关系。

7. 标准RNN的BPTT推导。
    h(t) = tanh(ux+wh(t-1)+b)
    y(t) = sigmoid(vh(t)+c)
    u,v,w为权重矩阵

8. 标准LSTM改进，以及为什么LSTM可以解决长期依赖的问题？
    加入了三个门，避免了层数增加梯度消失的问题。


9. GRU结构

10. transformer结构
    self-attention机制


11.word2vec的负采样以及实现的详细过程。第一个单词怎么处理的？

12. 为什么要用BERT+Bilstm进行预测？

13. crf的相关理论
