百度：
一面面经（1h）：
    1. 自我介绍
    2. 如何进行模型融合，比如logistic回归和随机森林融合，有没有创新？
    3. 如何处理样本不平衡？除了过采样和欠采样有没有甚麽别的方法？
       数据、模型和评估方法
       3.1 采样--过采样和欠采样
       3.2 增加不同的权重--更改损失函数
       3.3 评估标准--ROC或者F1
    4. 如何处理过拟合？
       4.1 增大样本量
       4.2 dropout
       4.3 添加正则项
       4.4 early stopping
    5. 主成分分析？--方差-协方差矩阵，降维
    6. 如何进行特征选择？
       6.1 过滤法：利用相关系数
       6.2 包裹法：逐步回归，验证不同特征子集的效果
       6.3 嵌入法：L1正则和L2正则
    7. 多头注意力的注意力是如何实现的？
    8. 为什么BERT要加双向LSTM？
    9. word2vec的两个模型以及层次softmax和负采样技术。两种词向量分别适用于甚麽样的任务？
        skip-gram与word2vec的对比：
            1. skip-gram需要预测的次数比较多
            2. skip-gram利用周围的k个词更新中心词的词向量，cbow只利用中心词更新周围的词，因此，skip-gram的效果比cbow稍微好点
            3. skip-gram也是利用滑动窗口进行探索的
    10. softmax是什么？
    11. 交叉熵公式？-- -ylog(y_predict)
    12. BERT与encoder-decoder的区别？--通过mask language model实现双向语言模型
    13. BERT的位置编码公式和损失函数是什么？--位置编码是自己学习的，损失函数是masked语言模型+下一句的分类损失函数
    14. BERT是怎末mask的？为什么要这样做？
        选择15%的tokens，随机mask80%, 随机替换10%，10%保持不变
        为什么要进行mask？--- 要训练双向语言模型，就会碰见自己看见自己的问题，利用mask进行遮挡
        为什么没有全部mask?---在微调阶段中没有mask标记
        为什么要随机替换10%的单词？---保持token的分布式特征，加入任意单词可以看作是引入了噪声，使模型更加稳定。
    15. bert时如何训练双向语言模型的？--通过mask language model实现的，借鉴CBOW的思想，将中心词mask
    16. 在线编程
        16.1 判断链表中是否存在环？如果存在返回交点，如果没有，返回空值
        16.2 最大连续子序列求和，忽略了数组中元素全为负整数的情况。
    17. 你还有什么想问的？
        17.1 目前nlp方向有什么新技术？
        17.2 推荐文本的相关内容

二面面经（1h）：
    1. 自我介绍
    2. 对BERT的理解
    3. 简历信息抽取项目，如何进行的？--具体细节
    4. 有没有调研过其它方法？
    5. LR和随机森林有了解吗？随机森林与gbdt的区别？
    6. gbdt是如何确定特征重要性的？
       特征重要性指标评估三种常用的方式：
           1. gain 增益--不确定性的变化程度，即该特征对损失函数的影响
              意味着相应的特征对通过对模型中的每个树采取每个特征的贡献而计算出的模型的相对贡献。
              与其他特征相比，此度量值的较高值意味着它对于生成预测更为重要。
           2. cover--利用该特征区分的叶节点中的样本数量来揭示特征重要性，如果该特征比较重要，那麽该特征越
              覆盖度量指的是与此功能相关的观测的相对数量。例如，如果您有100个观察值，4个特征和3棵树，
              并且假设特征1分别用于决定树1，树2和树3中10个，5个和2个观察值的叶节点;那么该度量将计算此功能
              的覆盖范围为10 + 5 + 2 = 17个观测值。这将针对所有4项功能进行计算，并将以17个百分比表示所有功能的覆盖指标。
           3. freq 频率（频率）是表示特定特征在模型树中发生的相对次数的百分比。
              在上面的例子中，如果feature1发生在2个分裂中，1个分裂和3个分裂在每个树1，树2和树3中;
              那么特征1的权重将是2 + 1 + 3 = 6。特征1的频率被计算为其在所有特征的权重上的百分比权重。
       6.1 随机森林（Random Forest）
           用袋外数据 (OOB) 做预测。随机森林在每次重抽样建立决策树时，都会有一些样本没有被选中，那么就可以用这些样本去做交叉验证，
           它可以不用做交叉验证，直接用oob _score_去对模型性能进行评估。
           具体的方法就是：
               1. 对于每一棵决策树，用OOB 计算袋外数据误差，记为 errOOB1；
               2. 然后随机对OOB所有样本的特征i加入噪声干扰，再次计算袋外数据误差，记为errOOB2；
               3. 假设有N棵树，特征i的重要性为sum(errOOB2-errOOB1)/N;
           如果加入随机噪声后，袋外数据准确率大幅下降，说明这个特征对预测结果有很大的影响，进而说明它的重要程度比较高
       6.2 梯度提升树（GBDT）
           主要是通过计算特征i在单棵树中重要度的平均值，计算公式如下：
           其中，M是树的数量。特征i在单棵树的重要度主要是通过计算按这个特征i分裂之后损失的减少值
           其中，L是叶子节点的数量，L-1就是非叶子结点的数量。
       6.3 XGboost
           XGboost是通过该特征每棵树中分裂次数的和去计算的，比如这个特征在第一棵树分裂1次，
           第二棵树2次……，那么这个特征的得分就是(1+2+...)。
    7. 如何看待加班？
    8. 会觉得同事压榨你吗？
    9. 如何去除重复文本？
    10. 如果有一个作者和一篇文章，如何帮助该作者寻找合适的用户？--用户的角度，作者的角度，文章的角度
    11. 如何判断作者与作者之间的相似度？--基本信息，文章，粉丝，行为习惯
    12. 实习过程中有没有甚麽比较大的变化？
    13. 如何从大批量数据中选择top10?
    14. 对hive的理解
    15. 主要有推荐、搜索和文本理解三个方向，大多是推荐和搜索，你对什么比较感兴趣？
    16. 在线编程
        16.1 设计一个抽奖的程序，如何保证抽到一等奖的概率是0.1，二等奖的概率是0.2，三等奖的概率是0.7？
        16.2 跳台阶代码
    17. 你还有甚麽想问的？
        17.1 如何识别标题党？--目前是机审+人工审核，机审效果较差
        17.2 推荐系统的相关内容。
三面面经：
    1. 实习为什么不在一个地方？
    2. 职业规划
    3. 跟团队里面的同事相处的如何？有没有什么矛盾的事情？
    4. 跟你的leader有没有意见不一致的地方，怎末处理的？
    5. 为什么这末晚才开始找工作？
    6. 你对百度的看法？
    7. 有没有压力比较大的地方？怎末处理的？---头脑风暴+加班加点+沟通
    8. 说一下你的优缺点。
    9. 你还有什么想问的？
       搜索策略，推荐策略，内容和策略理解。
人才测评：性格测试+6道语言理解行测题
HR面：

同花顺：
    一面：
        同花顺一面是机器人录音面试，侧重于机器学习和项目经历。
    二面：
        1.自我介绍
        2.介绍平安的命名实体识别项目
        3.介绍小浆实习内容
        4.贝叶斯公式
        5.介绍深度学习的其它内容
        6.如何提升模型的精度和减慢模型的运行时间?
        如果通过可能会有HR联系我，还有一轮视频面试。
    三面：
        1. 头条项目
        2. 平安项目
        3. 为什么不考虑留下?
        4. 有没有兴趣做推荐算法?
    HR面试：
        薪资
        期望工作地点
        拿的offer