
1.激活函数的选择
    理论上只要是连续可导的函数都可以作为激活函数。
    常用的激活函数：sigmoid函数和relu函数。
    sigmoid：不是关于原点中心对称，梯度下降法求最优解时，呈现之字形变化，收敛速度较慢。
             需要进行幂运算，消耗时间。
             容易达到饱和。(-10,10)，容易造成梯度消失。
    relu: y = max(0,x)
    可以解决梯度消失的问题，收敛速度较快。

2.梯度下降法的对比（adm,mini-batch) adm不理解
    以平方损失为例，梯度下降是对所有的样本求平均进行进行梯度更新。
    随机梯度下降：每次用一个样本近似所有的样本，进行梯度更新。
        优点：1.加入了噪声，提高了泛化误差。
              2.计算速度快。
        缺点：1.不收敛，在最小值附近波动。
              2.容易陷入局部最小值点。
    mini-batch梯度下降：随机选择部分样本进行更新，要对学习率衰减，刚开始希望步长较大，后面希望变小。

3.batchsize的选择，是不是越大越好？
    不是，较大的batchsize计算速度加快，容易使模型收敛在局部极小值点；
    较小的batch相当于认为加入噪声，计算速度比较慢。

4.梯度爆炸和梯度消失
    原因：1.权重初始值设置值过大。
          2.网络太复杂，反向传播用到链式法则。
          3.激活函数的选择不合适。
    解决方法：
          1.权重正则化，添加正则项。
          2.梯度剪切，设置梯度阈值。
          3.更换激活函数。
          4.利用LSTM的门结构。
          5.预训练加微调（少用）
